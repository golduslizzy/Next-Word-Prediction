import random
import pickle

import numpy as np
import pandas as pd
from nltk.tokenize import RegexpTokenizer

from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Activation
from tensorflow.keras.optimizers import RMSprop
text_df=pd.read_csv("Next Word Prediction Train dataset.csv")
text_df
text_df['Quotes'] = text_df['Quotes'].astype(str).str.lower()

tokenizer = RegexpTokenizer(r'\w+')
tokens = tokenizer.tokenize(' '.join(text_df['Quotes']))
tokens
unique_tokens = np.unique(tokens)
unique_token_index = {token: idx for idx, token in enumerate(unique_tokens)}
unique_token_index
n_words=10
input_words=[]
next_words=[]

for i in range(len(tokens)-n_words):
  input_words.append(tokens[i:i+n_words])
  next_words.append(tokens[i+n_words])
next_words
input_words
x=np.zeros((len(input_words),n_words,len(unique_tokens)),dtype=bool)
y=np.zeros((len(next_words),len(unique_tokens)),dtype=bool)
x
y
for i, words in enumerate(input_words):
  for j, word in enumerate(words):
    x[i,j,unique_token_index[word]]=1
  y[i, unique_token_index[next_words[i]]]=1
model=Sequential()
model.add(LSTM(128, input_shape=(n_words, len(unique_tokens)), return_sequences=True))
model.add(LSTM(128))
model.add(Dense(len(unique_tokens)))
model.add(Activation("softmax"))
model.compile(loss="categorical_crossentropy",optimizer=RMSprop(learning_rate=0.01),metrics=["accuracy"])
model.fit(x,y,batch_size=128,epochs=10,shuffle=True)
model.save("mymodel.h5")
def predict_next_word(input_text, n_best):
  input_text = input_text.lower()
  x = np.zeros((1, n_words,len(unique_tokens)))
  for i, word in enumerate(input_text.split()):
    x[0,i,unique_token_index[word]]=1

  predictions=model.predict(x)[0]
  return np.argpartition(predictions, n_best)[n_best:]
possible=predict_next_word("they will have to watch this and they",5)
possible
print([unique_tokens[idx] for idx in possible])
def generate_text(input_text,text_length,creativity=3):
  word_sequence=input_text.split()
  current=0
  for _ in range(text_length):
    sub_sequence=" ".join(tokenizer.tokenize(" ".join(word_sequence).lower())[current:current+n_words])
    try:
      choice=unique_tokens[random.choice[predict_next_word(sub_Sequence, creativity)]]
    except:
      choice=random.choice(unique_tokens)
    word_sequence.append(choice)
    current+=1
  return " ".join(word_sequence)
generate_text("they will have to watch this andÂ they",100,5)
